# -*- coding: utf-8 -*-
"""MLProjectFinal Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TmAey9w33C0K71EorVq7zngGL6yPI73j
"""

#data manipulation
import pandas as pd
import numpy as np
import pickle
import re
import string
import zipfile

#methods and stopwords text processing
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

#machine learning libraries
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
# plotting
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# sklearn
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report

from keras.utils import to_categorical, plot_model
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from keras.layers import Dense, Flatten, LSTM, Conv1D,Bidirectional, MaxPooling1D, Dropout, Activation, BatchNormalization
from keras.models import Model, Sequential, model_from_json, load_model
from keras.optimizers import Adam, SGD
from keras.layers import Embedding,SpatialDropout1D,RNN,SimpleRNN,Layer
from keras.initializers import Constant
import keras

from tqdm import tqdm
from nltk import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from google.colab import drive
drive.mount('/content/drive')

"""**Stopwords**

"""

#creating a stopword set
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stop = stop_words
# Defining dictionary containing all emojis with their meanings.
emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', 
          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',
          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\': 'annoyed', 
          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',
          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',
          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', ":'-)": 'sadsmile', ';)': 'wink', 
          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}

"""**Load the** **Dataset**"""

def load_dataset(filepath, cols):
  df = pd.read_csv(filepath, encoding='ISO-8859-1')
  df.columns = cols 
  return df

"""**Preprocessing**"""

def preprocess_tweets(tweet):
  processedText = []

  #for tweet in textdata:

  # convert all the txt to lowercase
  tweet = tweet.lower()

  # remove any urls
  # tweet = re.sub(r"http\S+|www\S+|https\S+", "", tweet, flags=re.MULTILINE)
  tweet = re.sub(r"http\S+|www\S+|https\S+", "", tweet)

  #remove punctuations
  tweet = tweet.translate(str.maketrans("", "", string.punctuation))

  # remove user @ references and '#' from tweet
  tweet = re.sub(r'\@\w+|\#', "", tweet)

  #remove stopwords
  tweet_tokens = word_tokenize(tweet)
  filtered_words = [ word for word in tweet_tokens if word not in stop_words]

  #stemming
  ps = PorterStemmer()
  stemmed_words = [ps.stem(w) for w in filtered_words]

  #lemmatizing
  lemmatizer = WordNetLemmatizer()
  lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]
  #lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in filtered_words]

      #processedText.append((lemma_words))
        
  #return processedText

  return " ".join(lemma_words)

# Importing the dataset

#dataset = load_dataset('/content/drive/My Drive/Colab Notebooks/MLProject/training.1600000.processed.noemoticon.csv',
 #            ["sentiment", "ids", "date", "flag", "user", "text"])
zf = zipfile.ZipFile('/content/drive/My Drive/training.1600000.processed.noemoticon.csv.zip') 
DATASET_ENCODING = "ISO-8859-1"
DATASET_COLUMNS  = ["sentiment", "ids", "date", "flag", "user", "text"]
dataset = pd.read_csv(zf.open('training.1600000.processed.noemoticon.csv'), encoding=DATASET_ENCODING, names=DATASET_COLUMNS )
# Removing the unnecessary columns.
dataset = dataset[['sentiment','text']]

# Replacing the values to ease understanding.
dataset['sentiment'] = dataset['sentiment'].replace(4,1)

# Plotting the distribution for dataset.
ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',
                                               legend=False)
ax.set_xticklabels(['Negative','Positive'], rotation=0)

# Storing data in lists.
text, sentiment = list(dataset['text']), list(dataset['sentiment'])

dataset

def preprocess(textdata):    
    processedText = []
    
    # Create Lemmatizer and Stemmer.
    wordLemm = WordNetLemmatizer()
    
    # Defining regex patterns.
    urlPattern        = r"((http://)[^ ]*|(https://)[^ ]*|( www\.)[^ ]*)"
    userPattern       = '@[^\s]+'
    alphaPattern      = "[^a-zA-Z0-9]"
    sequencePattern   = r"(.)\1\1+"
    seqReplacePattern = r"\1\1"
    
    for tweet in textdata:
        tweet = tweet.lower()
        
        # Replace all URls with 'URL'
        tweet = re.sub(urlPattern,' ',tweet)
        # Replace all emojis.
        for emoji in emojis.keys():
            tweet = tweet.replace(emoji, "EMOJI" + emojis[emoji])        
        # Replace @USERNAME to 'USER'.
        tweet = re.sub(userPattern,' ', tweet)        
        # Replace all non alphabets.
        tweet = re.sub(alphaPattern, " ", tweet)
        # Replace 3 or more consecutive letters by 2 letter.
        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)

        tweetwords = ''
        for word in tweet.split():
            # Checking if the word is a stopword.
            #if word not in stopwordlist:
            if len(word)>1:
                # Lemmatizing the word.
                word = wordLemm.lemmatize(word)
                tweetwords += (word+' ')
            
        processedText.append(tweetwords)
        
    return processedText

import time
t = time.time()
processedtext = preprocess(text)
print(f'Text Preprocessing complete.')
print(f'Time Taken: {round(time.time()-t)} seconds')

processedtext

processed = pd.read_csv("/content/sample_processed.csv")
processed = processed.dropna(axis=0)

X_train, X_test, y_train, y_test = train_test_split(processed['text'], processed['sentiment'],
                                                    test_size = 0.3,stratify =  processed['sentiment'], random_state = 0)

X_train

"""Word2vec"""

from gensim.models import Word2Vec

Embedding_dimensions = 100

# Creating Word2Vec training dataset.
Word2vec_train_data = list(map(lambda x: x.split(), X_train))

Word2vec_train_data

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Defining the model and training it.
# word2vec_model = Word2Vec(Word2vec_train_data,
#                  size=Embedding_dimensions,
#                  workers=8,
#                  min_count=5)
# 
# print("Vocabulary Length:", len(word2vec_model.wv.vocab))

vocab=list(word2vec_model.wv.vocab)

df_train = pd.DataFrame()
df_train['text'] = X_train
df_train['sentiment'] = y_train

x_train = pd.DataFrame()
for doc in df_train['text']:
  temp = pd.DataFrame()
  for w in doc.split(' '):
    try:
      word_vec = word2vec_model.wv[w]
      temp = temp.append(pd.Series(word_vec), ignore_index = True)
    except:
      pass
  a = temp.mean()
  x_train = x_train.append(a, ignore_index = True)

x_train.shape



x_train.to_csv("x_train.csv" ,  index=False)

x_train

X_train['sentiment']

df_test = pd.DataFrame()
df_test['text'] = X_test
df_test['sentiment'] = y_test

x_test = pd.DataFrame()
for doc in df_test['text']:
  temp_test = pd.DataFrame()
  for w in doc.split(' '):
    try:
      word_vec_t = word2vec_model.wv[w]
      temp_test = temp_test.append(pd.Series(word_vec_t), ignore_index = True)
    except:
      pass
  x = temp_test.mean()
  x_test = x_test.append(x, ignore_index = True)

x_test.shape



x_test.to_csv("x_testword2vec.csv" ,  index=False)



input_length = 60

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(filters="", lower=False, oov_token="<oov>")
tokenizer.fit_on_texts(processed['text'])

vocab_length = len(tokenizer.word_index) + 1
print("Tokenizer vocab length:", vocab_length)

X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)
X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)

print("X_train.shape:", X_train.shape)
print("X_test.shape :", X_test.shape)

embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))

for word, token in tokenizer.word_index.items():
    if word2vec_model.wv.__contains__(word):
        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)

print("Embedding Matrix Shape:", embedding_matrix.shape)

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding

embedding_layer = Embedding(input_dim = vocab_length, 
                                output_dim = Embedding_dimensions,
                                weights=[embedding_matrix], 
                                input_length=input_length,
                                trainable=False)

model = Sequential([
        embedding_layer,
        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),
        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),
        Conv1D(100, 5, activation='relu'),
        GlobalMaxPool1D(),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid'),
    ],
    name="Sentiment_Model")

Embedding_dimensions

embedding_layer = Embedding(input_dim = vocab_length, 
                                output_dim = Embedding_dimensions,
                                embeddings_initializer=Constant(embedding_matrix), 
                                input_length=input_length,
                                trainable=False)

model = Sequential([
        embedding_layer,
        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),
        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),
        Conv1D(100, 5, activation='relu'),
        GlobalMaxPool1D(),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid'),
    ],
    name="Sentiment_Model")

training_model = model
training_model.summary()

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]

training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = training_model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=10,
    validation_split=0.1,
    verbose=1,
)

training_model.evaluate(X_test,y_test)

embedding_layer = Embedding(input_dim = vocab_length, 
                                output_dim = Embedding_dimensions,
                                embeddings_initializer=Constant(embedding_matrix), 
                                input_length=input_length,
                                trainable=False)

model2 = Sequential([
        embedding_layer,
        Dropout(0.3),
        LSTM(20),
        Dropout(0.3),
        Dense(1,activation='sigmoid')
    ],
    name="Sentiment_Model2")

model2.summary()

model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = training_model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=1,
    validation_split=0.1,
    verbose=1,
)

model2.evaluate(X_test,y_test)

acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']
loss, val_loss = history.history['loss'], history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()







def model_Evaluate(model):
    
    # Predict values for Test dataset
    y_pred = model.predict(X_test)

    # Print the evaluation metrics for the dataset.
    print(classification_report(y_test, y_pred))
    
    # Compute and plot the Confusion matrix
    cf_matrix = confusion_matrix(y_test, y_pred)

    categories  = ['Negative','Positive']
    group_names = ['True Neg','False Pos', 'False Neg','True Pos']
    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]

    labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)

    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',
                xticklabels = categories, yticklabels = categories)

    plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
    plt.ylabel("Actual values"   , fontdict = {'size':14}, labelpad = 10)
    plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

BNBmodel = BernoulliNB(alpha = 2)
BNBmodel.fit(X_train, y_train)
model_Evaluate(BNBmodel)

SVCmodel = LinearSVC()
SVCmodel.fit(X_train, y_train)
model_Evaluate(SVCmodel)

LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)
LRmodel.fit(X_train, y_train)
model_Evaluate(LRmodel)

from xgboost import XGBClassifier
xgbmodel = XGBClassifier(max_depth=50, n_estimators=10, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8)
xgbmodel.fit(X_train, y_train)
model_Evaluate(xgbmodel)

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(random_state=1, max_iter=10)
mlp = mlp.fit(X_train, y_train)
model_Evaluate(mlp)

from sklearn import svm
svm_clf= svm.SVC(kernel='rbf')
svm_clf = svm_clf.fit(X_train , y_train)
model_Evaluate(svm_clf)

y_pre = LRmodel.predict(X_test)

#False Positive
for i in range(0,100):
  if y_test[i] == 0 and y_pre[i] == 1:
    print(copy_test[i])

#False Negative
for i in range(0,100):
  if y_test[i] == 1 and y_pre[i] == 0:
    print(copy_test[i])